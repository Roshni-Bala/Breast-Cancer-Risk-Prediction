# -*- coding: utf-8 -*-
"""risk_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1cEl88z-ZEdzsKzTkZk207hnPxsKsg2

<h3>Breast Cancer Risk Prediciton</h3>
Data Source: University of California, Irvine (UCI) dataset

importing data with suitable headers
"""

import pandas as pd

titles = ['id',	'diagnosis',	'radius_mean','texture_mean',	'perimeter_mean',	'area_mean',	'smoothness_mean',	'compactness_mean',	'concavity_mean', 'concave_points_mean','symmetry_mean',	'fractal_dimension_mean','radius_stderr', 'texture_stderr', 'perimeter_stderr', 'area_stderr', 'smoothness_stderr', 'compactness_stderr', 'concavity_stderr', 'concave_points_stderr', 'symmetry_stderr', 'fractal_dimension_stderr','radius_w', 'texture_w', 'perimeter_w', 'area_w', 'smoothness_w', 'compactness_w', 'concavity_w', 'concave_points_w', 'symmetry_w', 'fractal_dimension_w']
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', names=titles)
print(df.shape)
df.head()

"""getting info about data - data type and count of values"""

df.info()

"""describing the data in detail - count, mean, std, max, min, quartiles"""

df.describe()

"""checking for null values if present"""

df.isnull().any()

"""finding unique diagnosis present in dataset"""

df.diagnosis.unique()

"""creating dataframe containing count of each type of diagnosis
<br>
here, B: Benign and M: Malignant
"""

dfgrp = df
df_diagnosis = dfgrp.groupby('diagnosis')
dfdiagsize = df_diagnosis.size()
print(dfdiagsize)

"""finding how much each data attribute is skewed"""

df.skew()



#create 3 different sections (mean, largest (worst), stderr)
df_mean = df.iloc[:,2:12]
df_stderr = df.iloc[:,12:22] 
df_worst =  df.iloc[:,22:32]
# print(df_mean)
# print("**")
# print(df_stderr)
# print("**")
# print(df_worst)

df_mean.hist(bins=10, figsize=(10, 10), color = 'g')

df_worst.hist(bins=10, figsize=(10, 10), color = 'g')

df_stderr.hist(bins=10, figsize=(10, 10), color = 'g', legend=True, grid=True)

"""dataframes including diagnosis column"""

import pandas as pd
df_meanD = pd.DataFrame(df['diagnosis'])
df_meanD = df_meanD.join(df_mean)
df_stderrD = pd.DataFrame(df['diagnosis'])
df_stderrD = df_stderrD.join(df_stderr)
df_worstD = pd.DataFrame(df['diagnosis'])
df_worstD = df_worstD.join(df_worst)

import seaborn as sns
import matplotlib.pyplot as plt
pairgrid = sns.PairGrid(df_meanD, hue='diagnosis')
pairgrid = pairgrid.map_diag(sns.histplot)
pairgrid = pairgrid.map_offdiag(sns.scatterplot)
pairgrid.add_legend()

import numpy as np

corr = df_mean.corr()
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
df1, ax = plt.subplots(figsize=(10, 8))
plt.title('Breast Cancer Feature Correlation')
cmap = sns.diverging_palette(15, 130, as_cmap=True) #custom colourmap
sns.heatmap(corr, vmax=1.2, square='square', cmap=cmap, mask=mask, ax=ax,annot=True, fmt='.3g', linewidths=1)

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
df_arr = df['diagnosis']
df_arr = labelencoder.fit_transform(df_arr)
df['diagnosis'] = df_arr
df.head()

from sklearn.model_selection import train_test_split
# to find df['diagnosis'] and to use all other attr to predict diagnosis
df_ref = df
#df.head()
df_ref.head()
df_ref = df_ref.drop(columns = ['id', 'diagnosis'])
X = df_ref
print(X.head())
#df_ref.head()
#X = df.to_numpy()
#y = df['diagnosis'].to_numpy()
y = df['diagnosis']
print(y.head())
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=10)
X.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape

# from sklearn.preprocessing import StandardScaler
# scaler = StandardScaler()
# Xstd = scaler.fit_transform(X)
# Xstd



df

# #principal component analysis
# from sklearn.decomposition import PCA
# pca = PCA(n_components = 10)
# fitp = pca.fit(Xstd)
# print(Xstd)
# Xpca = pca.transform(Xstd)

# df_pca = pd.DataFrame()
# df_pca['radius_mean'] = Xpca[:,0]
# df_pca['texture_mean'] = Xpca[:,1]
# df_pca.head()
# plt.plot(df_pca['radius_mean'][df.diagnosis == 'M'], df_pca['texture_mean'][df.diagnosis == 'M'], 'o', color = 'r')
# plt.plot(df_pca['radius_mean'][df.diagnosis == 'B'], df_pca['texture_mean'][df.diagnosis == 'B'], 'o', color = 'b')
# plt.show()

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

from sklearn.decomposition import PCA

pca_t = PCA(n_components = 30)
pca_t.fit_transform(X)

explained_variance = pca_t.explained_variance_ratio_

print(explained_variance)

plt.plot(explained_variance)
plt.ylabel("Eigen Values")
plt.xlabel("Number of Components")
plt.legend(['Eigan Values from Principal Component Analysis'])
plt.show()

new_pca_com = 5
pca = PCA(n_components = new_pca_com)
principalcomps = pca.fit_transform(X)

df_pca = pd.DataFrame(data = principalcomps, columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])
df_pca1 = pd.concat([df_pca, df[['diagnosis']]], axis = 1)
df_pca1.head()

print('Explained variation per principal component = ', pca.explained_variance_ratio_)
print('Information loss due to PCA: ', ((1-np.sum(pca.explained_variance_ratio_))*100), '%')

fig = plt.figure(figsize = (5,5))
ax= fig.add_subplot(1,1,1)
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_title('P1 and P2 of 5 Components')
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==0], df_pca1['PC2'][df_pca1['diagnosis']==0],c='g', s = 50)
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==1], df_pca1['PC2'][df_pca1['diagnosis']==1],c='r', s = 50)
ax.legend(['Benign','Malignant'])
ax.grid()

fig = plt.figure(figsize = (5,5))
ax= fig.add_subplot(1,1,1)
ax.set_xlabel('PC1')
ax.set_ylabel('PC4')
ax.set_title('P1 and P4 of 5 Components')
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==0], df_pca1['PC4'][df_pca1['diagnosis']==0],c='g', s = 50)
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==1], df_pca1['PC4'][df_pca1['diagnosis']==1],c='r', s = 50)
ax.legend(['Benign','Malignant'])
ax.grid()

fig = plt.figure(figsize = (5,5))
ax= fig.add_subplot(1,1,1)
ax.set_xlabel('PC1')
ax.set_ylabel('PC3')
ax.set_title('P1 and P3 of 5 Components')
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==0], df_pca1['PC3'][df_pca1['diagnosis']==0],c='g', s = 50)
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==1], df_pca1['PC3'][df_pca1['diagnosis']==1],c='r', s = 50)
ax.legend(['Benign','Malignant'])
ax.grid()

fig = plt.figure(figsize = (5,5))
ax= fig.add_subplot(1,1,1)
ax.set_xlabel('PC1')
ax.set_ylabel('PC5')
ax.set_title('P1 and P5 of 5 Components')
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==0], df_pca1['PC5'][df_pca1['diagnosis']==0],c='g', s = 30)
ax.scatter(df_pca1['PC1'][df_pca1['diagnosis']==1], df_pca1['PC5'][df_pca1['diagnosis']==1],c='r', s = 30)
ax.legend(['Benign','Malignant'])
ax.grid()

#svm
#decision tree
#k neighbours
#random forest classifier

from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_train, y_train)

X_test.head()

pred_lm = reg.predict(X_test)
pred_bin_lm = pred_lm
print(pred_bin_lm)


np.place(pred_bin_lm, pred_bin_lm>=0.5, 1)
np.place(pred_bin_lm, pred_bin_lm<0.5, 0)
print(pred_bin_lm)

df_pred_lm = pd.DataFrame({'Predicted':pred_lm, 'Actual': y_test})
df_pred_lm.head(10)

from sklearn import metrics
print("Mean absolute error: ", metrics.mean_absolute_error(y_test, pred_bin_lm))
print("M")

from sklearn.metrics import confusion_matrix
labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']

confmat = confusion_matrix(y_test, pred_bin_lm)
sns.heatmap(confmat, square = True, annot = True , cmap = 'Greens', fmt = 'd', cbar = True)
# x axis - predicted - 0:false ; 1:true
# y axis - actual val- 0:negative ; 1:positive

accuracy_lm = metrics.accuracy_score(y_test, pred_bin_lm)
print("Accuracy of Linear Regression: ", round(accuracy_lm*100, 4), '%')





